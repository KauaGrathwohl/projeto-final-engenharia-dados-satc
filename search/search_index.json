{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Projeto Final Engenharia de Dados - SATC Bem-vindo Bem-vindo \u00e0 documenta\u00e7\u00e3o do projeto final da mat\u00e9ria de Engenharia de Dados na faculdade UniSatc. Este projeto tem como objetivo aplicar os conhecimentos adquiridos ao longo da mat\u00e9ria para desenvolver uma pipeline de dados. A documenta\u00e7\u00e3o aqui presente visa fornecer uma vis\u00e3o detalhada de todas as etapas do projeto. Grupo de Desenvolvimento - Kau\u00e3 Machado Grathwohl - Thiago Larangeira De Souza - Filipe Milaneze de Aguiar Estrutura da Documenta\u00e7\u00e3o Projeto Pipeline de Dados","title":"Home"},{"location":"#projeto-final-engenharia-de-dados-satc","text":"","title":"Projeto Final Engenharia de Dados - SATC"},{"location":"#bem-vindo","text":"Bem-vindo \u00e0 documenta\u00e7\u00e3o do projeto final da mat\u00e9ria de Engenharia de Dados na faculdade UniSatc. Este projeto tem como objetivo aplicar os conhecimentos adquiridos ao longo da mat\u00e9ria para desenvolver uma pipeline de dados. A documenta\u00e7\u00e3o aqui presente visa fornecer uma vis\u00e3o detalhada de todas as etapas do projeto.","title":"Bem-vindo"},{"location":"#grupo-de-desenvolvimento","text":"- Kau\u00e3 Machado Grathwohl - Thiago Larangeira De Souza - Filipe Milaneze de Aguiar","title":"Grupo de Desenvolvimento"},{"location":"#estrutura-da-documentacao","text":"Projeto Pipeline de Dados","title":"Estrutura da Documenta\u00e7\u00e3o"},{"location":"configuracao/","text":"Configura\u00e7\u00f5es Conta Azure Todo o projeto depende da Azure ent\u00e3o voce deve ter acesso a uma conta seja a gratuita ou paga. Para Criar uma conta se dirija ao Site Oficial da Azure e clique em criar uma conta ou comece a usar. Voc\u00ea tambem ira precisar baixar o CLI do azure Logando no Azure CLI Rodar esse comando para logar az login Listar dados da conta e do resource group, os dados aqui seram usados posteriormente az account show -o table az group list -o table Inciando os servi\u00e7os Azure 1. SGDB SQL Server (MMSQL) na Azure Navegar at\u00e9 iac/sqlserver Alterar os valores das variaveis no arquivo variables.tf variable \"resource_group_name\" { ... default = \"Inserir Aqui o nom do seu resource group\" } Rodar os comandos do terraform para subir uma instacia do SQL server no seu azure terrafrom init terraform apply Assim o seu sql sever vai estar configurado, caso precise deletar ele rodar: terrafrom destroy 2. Azure ADLS2 Navegar at\u00e9 ./iac/adls Alterar os valores das variaveis no arquivo variables.tf variable \"subscription_id\" { ... default = \"alterar pelo seu subscription id\" } Rodar os comandos do terraform para subir uma instacia do ADLS2 no seu azure terrafrom init terraform apply Assim o seu ADLS2 vai estar configurado, caso precise deletar ele rodar: terrafrom destroy 3. Databricks na Azure Navegar at\u00e9 ./iac/databricks Alterar os valores das variaveis no arquivo variables.tf variable \"azure_client_id\" { ... default = \"alterar pelo seu azure client id\" } variable \"azure_client_secret\" { ... default = \"alterar pelo seu azure client secret\" } variable \"azure_tenant_id\" { ... default = \"alterar pelo seu azure tenant id\" } variable \"subscription_id\" { ... default = \"alterar pelo seu subscription id\" } Rodar os comandos do terraform para subir uma instacia do databrick no seu azure terrafrom init terraform apply Assim o seu databricks vai estar configurado, caso precise deletar ele rodar: terrafrom destroy Configura\u00e7\u00f5es do MKDOCS 1. Instalar as libs do mkdocs pip install mkdocs pip install mkdocs-ivory 2. Entrar na pasta /docs 3. Rodar localmente mkdocs serve 4. Build em uma pasta mkdocs build 5. Deploy para o github Ele \u00e9 criado na branch gh-pages mkdocs gh-deploy Configura\u00e7\u00f5es do Gerador de dados Na pasta notebooks/gerador rodar os comandos pip install -r requirements.txt Executar o script python gerador.py","title":"Configura\u00e7\u00e3o"},{"location":"configuracao/#configuracoes","text":"","title":"Configura\u00e7\u00f5es"},{"location":"configuracao/#conta-azure","text":"Todo o projeto depende da Azure ent\u00e3o voce deve ter acesso a uma conta seja a gratuita ou paga. Para Criar uma conta se dirija ao Site Oficial da Azure e clique em criar uma conta ou comece a usar. Voc\u00ea tambem ira precisar baixar o CLI do azure","title":"Conta Azure"},{"location":"configuracao/#logando-no-azure-cli","text":"Rodar esse comando para logar az login Listar dados da conta e do resource group, os dados aqui seram usados posteriormente az account show -o table az group list -o table","title":"Logando no Azure CLI"},{"location":"configuracao/#inciando-os-servicos-azure","text":"","title":"Inciando os servi\u00e7os Azure"},{"location":"configuracao/#1-sgdb-sql-server-mmsql-na-azure","text":"Navegar at\u00e9 iac/sqlserver Alterar os valores das variaveis no arquivo variables.tf variable \"resource_group_name\" { ... default = \"Inserir Aqui o nom do seu resource group\" } Rodar os comandos do terraform para subir uma instacia do SQL server no seu azure terrafrom init terraform apply Assim o seu sql sever vai estar configurado, caso precise deletar ele rodar: terrafrom destroy","title":"1. SGDB SQL Server (MMSQL) na Azure"},{"location":"configuracao/#2-azure-adls2","text":"Navegar at\u00e9 ./iac/adls Alterar os valores das variaveis no arquivo variables.tf variable \"subscription_id\" { ... default = \"alterar pelo seu subscription id\" } Rodar os comandos do terraform para subir uma instacia do ADLS2 no seu azure terrafrom init terraform apply Assim o seu ADLS2 vai estar configurado, caso precise deletar ele rodar: terrafrom destroy","title":"2. Azure ADLS2"},{"location":"configuracao/#3-databricks-na-azure","text":"Navegar at\u00e9 ./iac/databricks Alterar os valores das variaveis no arquivo variables.tf variable \"azure_client_id\" { ... default = \"alterar pelo seu azure client id\" } variable \"azure_client_secret\" { ... default = \"alterar pelo seu azure client secret\" } variable \"azure_tenant_id\" { ... default = \"alterar pelo seu azure tenant id\" } variable \"subscription_id\" { ... default = \"alterar pelo seu subscription id\" } Rodar os comandos do terraform para subir uma instacia do databrick no seu azure terrafrom init terraform apply Assim o seu databricks vai estar configurado, caso precise deletar ele rodar: terrafrom destroy","title":"3. Databricks na Azure"},{"location":"configuracao/#configuracoes-do-mkdocs","text":"1. Instalar as libs do mkdocs pip install mkdocs pip install mkdocs-ivory 2. Entrar na pasta /docs 3. Rodar localmente mkdocs serve 4. Build em uma pasta mkdocs build 5. Deploy para o github Ele \u00e9 criado na branch gh-pages mkdocs gh-deploy","title":"Configura\u00e7\u00f5es do MKDOCS"},{"location":"configuracao/#configuracoes-do-gerador-de-dados","text":"Na pasta notebooks/gerador rodar os comandos pip install -r requirements.txt Executar o script python gerador.py","title":"Configura\u00e7\u00f5es do Gerador de dados"},{"location":"pipeline/","text":"Pipeline de Dados Introdu\u00e7\u00e3o Uma pipeline de dados \u00e9 um conjunto de processos que extrai, transforma e carrega dados de v\u00e1rias fontes para um destino final. Este documento descreve a cria\u00e7\u00e3o de uma pipeline de dados robusta, incluindo o tratamento das camadas landing, bronze, silver e gold, e a organiza\u00e7\u00e3o do workflow via Databricks. Cria\u00e7\u00e3o da Pipeline de Dados 1. Extra\u00e7\u00e3o de Dados A extra\u00e7\u00e3o de dados \u00e9 o primeiro passo na pipeline de dados. Os dados podem ser extra\u00eddos de v\u00e1rias fontes. Em nosso projeto ele \u00e9 extra\u00eddo de um banco sequencial (SQL Server), e logo ap\u00f3s, cada tabela \u00e9 transformada em arquivos CSV. 2. Transforma\u00e7\u00e3o de Dados A transforma\u00e7\u00e3o de dados envolve aplicar tratamentos e transforma\u00e7\u00f5es aos dados originais, e salv\u00e1-los em formatos de dados Delta Tables Tratamento das Camadas 1. Camada Landing A camada landing \u00e9 onde os dados brutos s\u00e3o inicialmente armazenados ap\u00f3s a extra\u00e7\u00e3o. Esta camada serve como um ponto de entrada para os dados na pipeline. Para essa camada s\u00e3o acessados os dados do banco de dados sequencial (SQL Server) hospenada na Azure e extrai os dados de cada tabela em csv para um container blob storage de nome landing no Azure ADLS2 seguindo as etapas: Conectar no SQL server Realizar um query para saber quantas tabelas existem no database Para cada tabela recuperar os seus conteudos e salvar em csv na camada landing Codigo de referencia 2. Camada Bronze A camada bronze \u00e9 onde os dados brutos s\u00e3o armazenados ap\u00f3s uma limpeza inicial. Esta camada \u00e9 usada para armazenar dados que ainda precisam de processamento adicional. Nela Recuperamos os dados salvos na camada landing em csv adicionamos metadados de processamento como data e nome de arquivo original caso tenha necessidade posteriormente e salvamos os dados na camada Bronze usando o formato delta table: Recuperar os dados da landing Adicionar dados extras e metadados Salvar na camada Bronze em formato delta table Codigo de referencia 3. Camada Silver A camada silver \u00e9 onde os dados s\u00e3o transformados e enriquecidos. Esta camada \u00e9 usada para armazenar dados que est\u00e3o prontos para an\u00e1lise. Nela ajustamos os dados pra qualquer trabalho futuro, ajustando incoerencias e definindo a melhor estrutura para todo os dados Recuperar dados da bronze Adicionar dados extras e metadados Ajustar estrutura, campos, nomes, da tebala e colunas Salvar os dados alterados na camada Silver no formato delta Codigo de referencia 4. Camada Gold A camada gold \u00e9 onde os dados finais e prontos para consumo s\u00e3o armazenados. Esta camada \u00e9 usada para armazenar dados que est\u00e3o prontos para serem usados em relat\u00f3rios e dashboards. Nela os dados s\u00e3o tratados e convertidos para um uso expecifico, no nosso caso passar para um modelo dimensional para analise em um dashboard do power BI Recuperar dados do silver Filtrar e converter para o modelo desejado Salvar em na camada gold em formato delta Codigo de referencia Organiza\u00e7\u00e3o do Workflow via Databricks Databricks \u00e9 uma plataforma de an\u00e1lise de dados que facilita a cria\u00e7\u00e3o e a gest\u00e3o de pipelines de dados. Abaixo est\u00e1 como foi organizado o workflow usando Databricks. Definir um cluster para rodar o workflow Adicionar em ordem as camadas landing, bronze, silver e gold Definir um schedule de execu\u00e7\u00e3o (nenhum definido para o projeto) Finalizar a cria\u00e7\u00e3o do workflow Codigo referencia Depois desse processo bastar executar ou esperar ser executado caso tenha um schedule Conclus\u00e3o Este documento forneceu uma vis\u00e3o geral da cria\u00e7\u00e3o de uma pipeline de dados, incluindo o tratamento das camadas landing, bronze, silver e gold, e a organiza\u00e7\u00e3o do workflow via Databricks. Seguindo estas etapas, voc\u00ea pode criar uma pipeline de dados robusta e eficiente.","title":"Pipeline"},{"location":"pipeline/#pipeline-de-dados","text":"","title":"Pipeline de Dados"},{"location":"pipeline/#introducao","text":"Uma pipeline de dados \u00e9 um conjunto de processos que extrai, transforma e carrega dados de v\u00e1rias fontes para um destino final. Este documento descreve a cria\u00e7\u00e3o de uma pipeline de dados robusta, incluindo o tratamento das camadas landing, bronze, silver e gold, e a organiza\u00e7\u00e3o do workflow via Databricks.","title":"Introdu\u00e7\u00e3o"},{"location":"pipeline/#criacao-da-pipeline-de-dados","text":"","title":"Cria\u00e7\u00e3o da Pipeline de Dados"},{"location":"pipeline/#1-extracao-de-dados","text":"A extra\u00e7\u00e3o de dados \u00e9 o primeiro passo na pipeline de dados. Os dados podem ser extra\u00eddos de v\u00e1rias fontes. Em nosso projeto ele \u00e9 extra\u00eddo de um banco sequencial (SQL Server), e logo ap\u00f3s, cada tabela \u00e9 transformada em arquivos CSV.","title":"1. Extra\u00e7\u00e3o de Dados"},{"location":"pipeline/#2-transformacao-de-dados","text":"A transforma\u00e7\u00e3o de dados envolve aplicar tratamentos e transforma\u00e7\u00f5es aos dados originais, e salv\u00e1-los em formatos de dados Delta Tables","title":"2. Transforma\u00e7\u00e3o de Dados"},{"location":"pipeline/#tratamento-das-camadas","text":"","title":"Tratamento das Camadas"},{"location":"pipeline/#1-camada-landing","text":"A camada landing \u00e9 onde os dados brutos s\u00e3o inicialmente armazenados ap\u00f3s a extra\u00e7\u00e3o. Esta camada serve como um ponto de entrada para os dados na pipeline. Para essa camada s\u00e3o acessados os dados do banco de dados sequencial (SQL Server) hospenada na Azure e extrai os dados de cada tabela em csv para um container blob storage de nome landing no Azure ADLS2 seguindo as etapas: Conectar no SQL server Realizar um query para saber quantas tabelas existem no database Para cada tabela recuperar os seus conteudos e salvar em csv na camada landing Codigo de referencia","title":"1. Camada Landing"},{"location":"pipeline/#2-camada-bronze","text":"A camada bronze \u00e9 onde os dados brutos s\u00e3o armazenados ap\u00f3s uma limpeza inicial. Esta camada \u00e9 usada para armazenar dados que ainda precisam de processamento adicional. Nela Recuperamos os dados salvos na camada landing em csv adicionamos metadados de processamento como data e nome de arquivo original caso tenha necessidade posteriormente e salvamos os dados na camada Bronze usando o formato delta table: Recuperar os dados da landing Adicionar dados extras e metadados Salvar na camada Bronze em formato delta table Codigo de referencia","title":"2. Camada Bronze"},{"location":"pipeline/#3-camada-silver","text":"A camada silver \u00e9 onde os dados s\u00e3o transformados e enriquecidos. Esta camada \u00e9 usada para armazenar dados que est\u00e3o prontos para an\u00e1lise. Nela ajustamos os dados pra qualquer trabalho futuro, ajustando incoerencias e definindo a melhor estrutura para todo os dados Recuperar dados da bronze Adicionar dados extras e metadados Ajustar estrutura, campos, nomes, da tebala e colunas Salvar os dados alterados na camada Silver no formato delta Codigo de referencia","title":"3. Camada Silver"},{"location":"pipeline/#4-camada-gold","text":"A camada gold \u00e9 onde os dados finais e prontos para consumo s\u00e3o armazenados. Esta camada \u00e9 usada para armazenar dados que est\u00e3o prontos para serem usados em relat\u00f3rios e dashboards. Nela os dados s\u00e3o tratados e convertidos para um uso expecifico, no nosso caso passar para um modelo dimensional para analise em um dashboard do power BI Recuperar dados do silver Filtrar e converter para o modelo desejado Salvar em na camada gold em formato delta Codigo de referencia","title":"4. Camada Gold"},{"location":"pipeline/#organizacao-do-workflow-via-databricks","text":"Databricks \u00e9 uma plataforma de an\u00e1lise de dados que facilita a cria\u00e7\u00e3o e a gest\u00e3o de pipelines de dados. Abaixo est\u00e1 como foi organizado o workflow usando Databricks. Definir um cluster para rodar o workflow Adicionar em ordem as camadas landing, bronze, silver e gold Definir um schedule de execu\u00e7\u00e3o (nenhum definido para o projeto) Finalizar a cria\u00e7\u00e3o do workflow Codigo referencia Depois desse processo bastar executar ou esperar ser executado caso tenha um schedule","title":"Organiza\u00e7\u00e3o do Workflow via Databricks"},{"location":"pipeline/#conclusao","text":"Este documento forneceu uma vis\u00e3o geral da cria\u00e7\u00e3o de uma pipeline de dados, incluindo o tratamento das camadas landing, bronze, silver e gold, e a organiza\u00e7\u00e3o do workflow via Databricks. Seguindo estas etapas, voc\u00ea pode criar uma pipeline de dados robusta e eficiente.","title":"Conclus\u00e3o"},{"location":"projeto/","text":"Projeto Final Engenharia de Dados - SATC Tecnologias Utilizadas - Python: Linguagem de programa\u00e7\u00e3o utilizada para desenvolvimento dos scripts. - Azure ADLS2: O Azure Data Lake Storage \u00e9 um conjunto de funcionalidades dedicadas \u00e0 an\u00e1lise de Big Data, Armazenamento de Blobs do Azure. - Databricks: Ferramenta para organiza\u00e7\u00e3o do workflow. - SQL Server: Banco de dados relacional utilizado para armazenamento dos dados. - PySpark: Biblioteca de processamento de dados distribu\u00eddos. Estrutura do Projeto \u251c\u2500 .gitignore \u251c\u2500 LICENSE.md \u251c\u2500 README.md \u251c\u2500 css \u2502 \u2514\u2500 extra.css \u251c\u2500 data \u2502 \u2514\u2500 raw \u2502 \u251c\u2500 dump.sql \u2502 \u251c\u2500 sakila-data.sql \u2502 \u251c\u2500 sakila-schema.sql \u2502 \u2514\u2500 schema.sql \u251c\u2500 desenho_arquitetura.excalidraw \u251c\u2500 desenho_arquitetura.png \u251c\u2500 docs \u2502 \u251c\u2500 docs \u2502 \u2502 \u251c\u2500 index.md \u2502 \u2502 \u251c\u2500 pipeline.md \u2502 \u2502 \u2514\u2500 projeto.md \u2502 \u2514\u2500 mkdocs.yml \u251c\u2500 iac \u2502 \u251c\u2500 .gitignore \u2502 \u251c\u2500 adls \u2502 \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u2502 \u251c\u2500 main.tf \u2502 \u2502 \u251c\u2500 output.tf \u2502 \u2502 \u251c\u2500 provider.tf \u2502 \u2502 \u2514\u2500 variables.tf \u2502 \u251c\u2500 databricks \u2502 \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u2502 \u251c\u2500 main.tf \u2502 \u2502 \u251c\u2500 output.tf \u2502 \u2502 \u251c\u2500 provider.tf \u2502 \u2502 \u2514\u2500 variables.tf \u2502 \u2514\u2500 sqlserver \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u251c\u2500 main.tf \u2502 \u251c\u2500 outputs.tf \u2502 \u251c\u2500 providers.tf \u2502 \u2514\u2500 variables.tf \u251c\u2500 notebooks \u2502 \u251c\u2500 camadas \u2502 \u2502 \u251c\u2500 1-landing.ipynb \u2502 \u2502 \u251c\u2500 2-bronze.ipynb \u2502 \u2502 \u251c\u2500 3-silver.ipynb \u2502 \u2502 \u251c\u2500 4-gold.ipynb \u2502 \u2502 \u251c\u2500 all.py \u2502 \u2502 \u2514\u2500 env.ipynb \u2502 \u2514\u2500 gerador \u2502 \u251c\u2500 .python-version \u2502 \u251c\u2500 gerador.py \u2502 \u2514\u2500 requirements.txt \u2514\u2500 requirements.txt","title":"Projeto"},{"location":"projeto/#projeto-final-engenharia-de-dados-satc","text":"","title":"Projeto Final Engenharia de Dados - SATC"},{"location":"projeto/#tecnologias-utilizadas","text":"- Python: Linguagem de programa\u00e7\u00e3o utilizada para desenvolvimento dos scripts. - Azure ADLS2: O Azure Data Lake Storage \u00e9 um conjunto de funcionalidades dedicadas \u00e0 an\u00e1lise de Big Data, Armazenamento de Blobs do Azure. - Databricks: Ferramenta para organiza\u00e7\u00e3o do workflow. - SQL Server: Banco de dados relacional utilizado para armazenamento dos dados. - PySpark: Biblioteca de processamento de dados distribu\u00eddos.","title":"Tecnologias Utilizadas"},{"location":"projeto/#estrutura-do-projeto","text":"\u251c\u2500 .gitignore \u251c\u2500 LICENSE.md \u251c\u2500 README.md \u251c\u2500 css \u2502 \u2514\u2500 extra.css \u251c\u2500 data \u2502 \u2514\u2500 raw \u2502 \u251c\u2500 dump.sql \u2502 \u251c\u2500 sakila-data.sql \u2502 \u251c\u2500 sakila-schema.sql \u2502 \u2514\u2500 schema.sql \u251c\u2500 desenho_arquitetura.excalidraw \u251c\u2500 desenho_arquitetura.png \u251c\u2500 docs \u2502 \u251c\u2500 docs \u2502 \u2502 \u251c\u2500 index.md \u2502 \u2502 \u251c\u2500 pipeline.md \u2502 \u2502 \u2514\u2500 projeto.md \u2502 \u2514\u2500 mkdocs.yml \u251c\u2500 iac \u2502 \u251c\u2500 .gitignore \u2502 \u251c\u2500 adls \u2502 \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u2502 \u251c\u2500 main.tf \u2502 \u2502 \u251c\u2500 output.tf \u2502 \u2502 \u251c\u2500 provider.tf \u2502 \u2502 \u2514\u2500 variables.tf \u2502 \u251c\u2500 databricks \u2502 \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u2502 \u251c\u2500 main.tf \u2502 \u2502 \u251c\u2500 output.tf \u2502 \u2502 \u251c\u2500 provider.tf \u2502 \u2502 \u2514\u2500 variables.tf \u2502 \u2514\u2500 sqlserver \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u251c\u2500 main.tf \u2502 \u251c\u2500 outputs.tf \u2502 \u251c\u2500 providers.tf \u2502 \u2514\u2500 variables.tf \u251c\u2500 notebooks \u2502 \u251c\u2500 camadas \u2502 \u2502 \u251c\u2500 1-landing.ipynb \u2502 \u2502 \u251c\u2500 2-bronze.ipynb \u2502 \u2502 \u251c\u2500 3-silver.ipynb \u2502 \u2502 \u251c\u2500 4-gold.ipynb \u2502 \u2502 \u251c\u2500 all.py \u2502 \u2502 \u2514\u2500 env.ipynb \u2502 \u2514\u2500 gerador \u2502 \u251c\u2500 .python-version \u2502 \u251c\u2500 gerador.py \u2502 \u2514\u2500 requirements.txt \u2514\u2500 requirements.txt","title":"Estrutura do Projeto"},{"location":"step/","text":"Passo a Passo 1. Instalar o Python 3.12.6 2. Instalar o Azure CLI 3. Instalar o Terraform 4. Instalar o Power BI Desktop 5. Logar na Azure 6. Iniciar todos os servi\u00e7os necessarios na Azure 7. Gerar os dados com o Gerador **8.","title":"Passo a Passo"},{"location":"step/#passo-a-passo","text":"1. Instalar o Python 3.12.6 2. Instalar o Azure CLI 3. Instalar o Terraform 4. Instalar o Power BI Desktop 5. Logar na Azure 6. Iniciar todos os servi\u00e7os necessarios na Azure 7. Gerar os dados com o Gerador **8.","title":"Passo a Passo"}]}