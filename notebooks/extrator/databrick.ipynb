{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalar com.microsoft.sqlserver:mssql-jdbc:10.2.0.jre8 no databricks \n",
    "\n",
    "%pip install azure-identity==1.19.0\n",
    "%pip install azure-storage-file-datalake==12.18.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the connection string for SQL Server\n",
    "jdbc_url = \"jdbc:sqlserver://sql-new-tapir.database.windows.net:1433;database=SampleDB\"\n",
    "connection_properties = {\n",
    "  \"user\" : \"azureadmin\",\n",
    "  \"password\" : \"Satc@1234\",\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "query = \"(SELECT table_name FROM INFORMATION_SCHEMA.TABLES) AS query\"\n",
    "\n",
    "# Read data from SQL Server\n",
    "df = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_properties)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# ADLS_ACCOUNT_NAME\n",
    "account_name = \"datalakec4056b1df75aa671\"\n",
    "\n",
    "# ADLS_FILE_SYSTEM_NAME\n",
    "file_system_name = \"landing-zone\"\n",
    "\n",
    "# ADLS_DIRECTORY_NAME\n",
    "directory_name = \"a\"\n",
    "\n",
    "# ADLS_SAS_TOKEN\n",
    "sas_token = \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2024-11-27T21:52:16Z&st=2024-11-27T13:52:16Z&spr=https&sig=7l8kb5ju4S24tgoyk%2F0Y8bLsu2zCeXHryL9rc4k1YM8%3D\"\n",
    "\n",
    "file_system_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{account_name}.dfs.core.windows.net\", \n",
    "    credential=sas_token,\n",
    "    api_version=\"2020-02-10\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    directory_client = file_system_client.get_file_system_client(file_system_name).get_directory_client(directory_name)\n",
    "    directory_client.create_directory()\n",
    "except ResourceExistsError:\n",
    "    print(f\"O diret칩rio '{directory_name}' j치 existe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tudo certo com o de baixo \n",
    "Ele adiciona as coisas na camadas landing-zone na pasta \"a\"\n",
    "Se faltar libs instalar com \n",
    "%pip install azure-identity==1.19.0\n",
    "%pip install azure-storage-file-datalake==12.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# ADLS_ACCOUNT_NAME\n",
    "account_name = \"datalakec4056b1df75aa671\"\n",
    "\n",
    "# ADLS_FILE_SYSTEM_NAME\n",
    "file_system_name = \"landing-zone\"\n",
    "\n",
    "# ADLS_DIRECTORY_NAME\n",
    "directory_name = \"a\"\n",
    "\n",
    "# ADLS_SAS_TOKEN\n",
    "sas_token = \"\"\n",
    "\n",
    "jdbc_url = \"jdbc:sqlserver://sql-new-tapir.database.windows.net:1433;database=SampleDB\"\n",
    "connection_properties = {\n",
    "  \"user\" : \"azureadmin\",\n",
    "  \"password\" : \"Satc@1234\",\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "schema = \"dbo\"\n",
    "\n",
    "query = f\"(SELECT table_name FROM INFORMATION_SCHEMA.TABLES WHERE table_schema = '{schema}') AS query\"\n",
    "df = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_properties).toPandas()\n",
    "\n",
    "file_system_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{account_name}.dfs.core.windows.net\", \n",
    "    credential=sas_token,\n",
    "    api_version=\"2020-02-10\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    directory_client = file_system_client.get_file_system_client(file_system_name).get_directory_client(directory_name)\n",
    "    directory_client.create_directory()\n",
    "except ResourceExistsError:\n",
    "    print(f\"O diret칩rio '{directory_name}' j치 existe.\")\n",
    "\n",
    "# Para cada tabela encontrada, ler os dados e carregar para o Azure Data Lake Storage\n",
    "for index, row in df.iterrows():\n",
    "    table_name = row[\"table_name\"]\n",
    "    query2 = f\"(SELECT * FROM {schema}.{table_name}) as query\"\n",
    "    df_table = spark.read.jdbc(url=jdbc_url, table=query2, properties=connection_properties).toPandas()\n",
    "    \n",
    "    file_client = directory_client.get_file_client(f\"{table_name}.csv\")\n",
    "    data = df_table.to_csv(index=False).encode()\n",
    "    file_client.upload_data(data, overwrite=True)\n",
    "    print(f\"Dados da tabela '{table_name}' carregados com sucesso.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
