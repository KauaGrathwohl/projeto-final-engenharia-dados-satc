{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Projeto Final Engenharia de Dados - SATC Bem-vindo Bem-vindo \u00e0 documenta\u00e7\u00e3o do projeto final da mat\u00e9ria de Engenharia de Dados na faculdade UniSatc. Este projeto tem como objetivo aplicar os conhecimentos adquiridos ao longo da mat\u00e9ria para desenvolver uma pipeline de dados. A documenta\u00e7\u00e3o aqui presente visa fornecer uma vis\u00e3o detalhada de todas as etapas do projeto. Grupo de Desenvolvimento - Kau\u00e3 Machado Grathwohl - Thiago Larangeira De Souza - Filipe Milaneze de Aguiar Estrutura da Documenta\u00e7\u00e3o Projeto Pipeline de Dados","title":"Home"},{"location":"#projeto-final-engenharia-de-dados-satc","text":"","title":"Projeto Final Engenharia de Dados - SATC"},{"location":"#bem-vindo","text":"Bem-vindo \u00e0 documenta\u00e7\u00e3o do projeto final da mat\u00e9ria de Engenharia de Dados na faculdade UniSatc. Este projeto tem como objetivo aplicar os conhecimentos adquiridos ao longo da mat\u00e9ria para desenvolver uma pipeline de dados. A documenta\u00e7\u00e3o aqui presente visa fornecer uma vis\u00e3o detalhada de todas as etapas do projeto.","title":"Bem-vindo"},{"location":"#grupo-de-desenvolvimento","text":"- Kau\u00e3 Machado Grathwohl - Thiago Larangeira De Souza - Filipe Milaneze de Aguiar","title":"Grupo de Desenvolvimento"},{"location":"#estrutura-da-documentacao","text":"Projeto Pipeline de Dados","title":"Estrutura da Documenta\u00e7\u00e3o"},{"location":"pipeline/","text":"Pipeline de Dados Introdu\u00e7\u00e3o Uma pipeline de dados \u00e9 um conjunto de processos que extrai, transforma e carrega dados de v\u00e1rias fontes para um destino final. Este documento descreve a cria\u00e7\u00e3o de uma pipeline de dados robusta, incluindo o tratamento das camadas landing, bronze, silver e gold, e a organiza\u00e7\u00e3o do workflow via Databricks. Cria\u00e7\u00e3o da Pipeline de Dados 1. Extra\u00e7\u00e3o de Dados A extra\u00e7\u00e3o de dados \u00e9 o primeiro passo na pipeline de dados. Os dados podem ser extra\u00eddos de v\u00e1rias fontes. Em nosso projeto ele \u00e9 extra\u00eddo de um banco sequencial (SQL Server), e logo ap\u00f3s, cada tabela \u00e9 transformada em arquivos CSV. 2. Transforma\u00e7\u00e3o de Dados A transforma\u00e7\u00e3o de dados envolve aplicar tratamentos e transforma\u00e7\u00f5es aos dados originais, e salv\u00e1-los em formatos de dados Delta Tables Tratamento das Camadas 1. Camada Landing A camada landing \u00e9 onde os dados brutos s\u00e3o inicialmente armazenados ap\u00f3s a extra\u00e7\u00e3o. Esta camada serve como um ponto de entrada para os dados na pipeline. Para essa camada s\u00e3o acessados os dados do banco de dados sequencial (SQL Server) hospenada na Azure e extrai os dados de cada tabela em csv para um container blob storage de nome landing no Azure ADLS2 seguindo as etapas: Conectar no SQL server jdbc_url = f\"jdbc:sqlserver://{DB_SERVER}:1433;database={DB_DATABASE}\" connection_properties = { \"user\" : DB_USER, \"password\" : DB_PASS, \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" } Realizar um query para saber quantas tabelas existem no database query = f\"(SELECT table_name FROM INFORMATION_SCHEMA.TABLES WHERE table_schema = '{DB_SCHEMA}') AS query\" df = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_properties).toPandas() Para cada tabela recuperar os seus conteudos e salvar em csv na camada landing for index, row in df.iterrows(): table_name = row[\"table_name\"] query2 = f\"(SELECT * FROM {DB_SCHEMA}.{table_name}) as query\" df_table = spark.read.jdbc(url=jdbc_url, table=query2, properties=connection_properties) df_table.write \\ .format(\"com.databricks.spark.csv\") \\ .option(\"header\", \"true\") \\ .mode(\"overwrite\") \\ .save(f\"/mnt/{ACCOUNT_NAME}/landing/{table_name}.csv\") print(f\"Dados da tabela '{table_name}' carregados com sucesso.\") 2. Camada Bronze A camada bronze \u00e9 onde os dados brutos s\u00e3o armazenados ap\u00f3s uma limpeza inicial. Esta camada \u00e9 usada para armazenar dados que ainda precisam de processamento adicional. Nela Recuperamos os dados salvos na camada landing em csv adicionamos metadados de processamento como data e nome de arquivo original caso tenha necessidade posteriormente e salvamos os dados na camada Bronze usando o formato delta table: Recuperar os dados da landing Adicionar dados extras e metadados Salvar na camada Bronze em formato delta table 3. Camada Silver A camada silver \u00e9 onde os dados s\u00e3o transformados e enriquecidos. Esta camada \u00e9 usada para armazenar dados que est\u00e3o prontos para an\u00e1lise. Nela ajustamos os dados pra qualquer trabalho futuro, ajustando incoerencias e definindo a melhor estrutura para todo os dados Recuperar dados da bronze Adicionar dados extras e metadados Ajustar estrutura, campos, nomes, da tebala e colunas Salvar os dados alterados na camada Silver no formato delta 4. Camada Gold A camada gold \u00e9 onde os dados finais e prontos para consumo s\u00e3o armazenados. Esta camada \u00e9 usada para armazenar dados que est\u00e3o prontos para serem usados em relat\u00f3rios e dashboards. Nela os dados s\u00e3o tratados e convertidos para um uso expecifico, no nosso caso passar para um modelo dimensional para analise em um dashboard do power BI Recuperar dados do silver Filtrar e converter para o modelo desejado Salvar em na camada gold em formato delta Organiza\u00e7\u00e3o do Workflow via Databricks Databricks \u00e9 uma plataforma de an\u00e1lise de dados que facilita a cria\u00e7\u00e3o e a gest\u00e3o de pipelines de dados. Abaixo est\u00e1 um exemplo de como organizar o workflow usando Databricks. 1. Cria\u00e7\u00e3o de um Notebook Crie um notebook no Databricks para organizar o seu workflow. Um notebook permite que voc\u00ea escreva e execute c\u00f3digo em blocos. 2. Configura\u00e7\u00e3o do Cluster Configure um cluster no Databricks para executar o seu notebook. Um cluster \u00e9 um conjunto de m\u00e1quinas virtuais que executam o seu c\u00f3digo. 3. Organiza\u00e7\u00e3o do Workflow Organize o seu workflow em etapas l\u00f3gicas dentro do notebook. Cada etapa deve corresponder a uma parte da pipeline de dados. 4. Agendamento do Workflow Use o Databricks Job Scheduler para agendar a execu\u00e7\u00e3o do seu notebook em intervalos regulares. Isso garante que a sua pipeline de dados seja executada automaticamente. Conclus\u00e3o Este documento forneceu uma vis\u00e3o geral da cria\u00e7\u00e3o de uma pipeline de dados, incluindo o tratamento das camadas landing, bronze, silver e gold, e a organiza\u00e7\u00e3o do workflow via Databricks. Seguindo estas etapas, voc\u00ea pode criar uma pipeline de dados robusta e eficiente.","title":"Pipeline"},{"location":"pipeline/#pipeline-de-dados","text":"","title":"Pipeline de Dados"},{"location":"pipeline/#introducao","text":"Uma pipeline de dados \u00e9 um conjunto de processos que extrai, transforma e carrega dados de v\u00e1rias fontes para um destino final. Este documento descreve a cria\u00e7\u00e3o de uma pipeline de dados robusta, incluindo o tratamento das camadas landing, bronze, silver e gold, e a organiza\u00e7\u00e3o do workflow via Databricks.","title":"Introdu\u00e7\u00e3o"},{"location":"pipeline/#criacao-da-pipeline-de-dados","text":"","title":"Cria\u00e7\u00e3o da Pipeline de Dados"},{"location":"pipeline/#1-extracao-de-dados","text":"A extra\u00e7\u00e3o de dados \u00e9 o primeiro passo na pipeline de dados. Os dados podem ser extra\u00eddos de v\u00e1rias fontes. Em nosso projeto ele \u00e9 extra\u00eddo de um banco sequencial (SQL Server), e logo ap\u00f3s, cada tabela \u00e9 transformada em arquivos CSV.","title":"1. Extra\u00e7\u00e3o de Dados"},{"location":"pipeline/#2-transformacao-de-dados","text":"A transforma\u00e7\u00e3o de dados envolve aplicar tratamentos e transforma\u00e7\u00f5es aos dados originais, e salv\u00e1-los em formatos de dados Delta Tables","title":"2. Transforma\u00e7\u00e3o de Dados"},{"location":"pipeline/#tratamento-das-camadas","text":"","title":"Tratamento das Camadas"},{"location":"pipeline/#1-camada-landing","text":"A camada landing \u00e9 onde os dados brutos s\u00e3o inicialmente armazenados ap\u00f3s a extra\u00e7\u00e3o. Esta camada serve como um ponto de entrada para os dados na pipeline. Para essa camada s\u00e3o acessados os dados do banco de dados sequencial (SQL Server) hospenada na Azure e extrai os dados de cada tabela em csv para um container blob storage de nome landing no Azure ADLS2 seguindo as etapas: Conectar no SQL server jdbc_url = f\"jdbc:sqlserver://{DB_SERVER}:1433;database={DB_DATABASE}\" connection_properties = { \"user\" : DB_USER, \"password\" : DB_PASS, \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" } Realizar um query para saber quantas tabelas existem no database query = f\"(SELECT table_name FROM INFORMATION_SCHEMA.TABLES WHERE table_schema = '{DB_SCHEMA}') AS query\" df = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_properties).toPandas() Para cada tabela recuperar os seus conteudos e salvar em csv na camada landing for index, row in df.iterrows(): table_name = row[\"table_name\"] query2 = f\"(SELECT * FROM {DB_SCHEMA}.{table_name}) as query\" df_table = spark.read.jdbc(url=jdbc_url, table=query2, properties=connection_properties) df_table.write \\ .format(\"com.databricks.spark.csv\") \\ .option(\"header\", \"true\") \\ .mode(\"overwrite\") \\ .save(f\"/mnt/{ACCOUNT_NAME}/landing/{table_name}.csv\") print(f\"Dados da tabela '{table_name}' carregados com sucesso.\")","title":"1. Camada Landing"},{"location":"pipeline/#2-camada-bronze","text":"A camada bronze \u00e9 onde os dados brutos s\u00e3o armazenados ap\u00f3s uma limpeza inicial. Esta camada \u00e9 usada para armazenar dados que ainda precisam de processamento adicional. Nela Recuperamos os dados salvos na camada landing em csv adicionamos metadados de processamento como data e nome de arquivo original caso tenha necessidade posteriormente e salvamos os dados na camada Bronze usando o formato delta table: Recuperar os dados da landing Adicionar dados extras e metadados Salvar na camada Bronze em formato delta table","title":"2. Camada Bronze"},{"location":"pipeline/#3-camada-silver","text":"A camada silver \u00e9 onde os dados s\u00e3o transformados e enriquecidos. Esta camada \u00e9 usada para armazenar dados que est\u00e3o prontos para an\u00e1lise. Nela ajustamos os dados pra qualquer trabalho futuro, ajustando incoerencias e definindo a melhor estrutura para todo os dados Recuperar dados da bronze Adicionar dados extras e metadados Ajustar estrutura, campos, nomes, da tebala e colunas Salvar os dados alterados na camada Silver no formato delta","title":"3. Camada Silver"},{"location":"pipeline/#4-camada-gold","text":"A camada gold \u00e9 onde os dados finais e prontos para consumo s\u00e3o armazenados. Esta camada \u00e9 usada para armazenar dados que est\u00e3o prontos para serem usados em relat\u00f3rios e dashboards. Nela os dados s\u00e3o tratados e convertidos para um uso expecifico, no nosso caso passar para um modelo dimensional para analise em um dashboard do power BI Recuperar dados do silver Filtrar e converter para o modelo desejado Salvar em na camada gold em formato delta","title":"4. Camada Gold"},{"location":"pipeline/#organizacao-do-workflow-via-databricks","text":"Databricks \u00e9 uma plataforma de an\u00e1lise de dados que facilita a cria\u00e7\u00e3o e a gest\u00e3o de pipelines de dados. Abaixo est\u00e1 um exemplo de como organizar o workflow usando Databricks.","title":"Organiza\u00e7\u00e3o do Workflow via Databricks"},{"location":"pipeline/#1-criacao-de-um-notebook","text":"Crie um notebook no Databricks para organizar o seu workflow. Um notebook permite que voc\u00ea escreva e execute c\u00f3digo em blocos.","title":"1. Cria\u00e7\u00e3o de um Notebook"},{"location":"pipeline/#2-configuracao-do-cluster","text":"Configure um cluster no Databricks para executar o seu notebook. Um cluster \u00e9 um conjunto de m\u00e1quinas virtuais que executam o seu c\u00f3digo.","title":"2. Configura\u00e7\u00e3o do Cluster"},{"location":"pipeline/#3-organizacao-do-workflow","text":"Organize o seu workflow em etapas l\u00f3gicas dentro do notebook. Cada etapa deve corresponder a uma parte da pipeline de dados.","title":"3. Organiza\u00e7\u00e3o do Workflow"},{"location":"pipeline/#4-agendamento-do-workflow","text":"Use o Databricks Job Scheduler para agendar a execu\u00e7\u00e3o do seu notebook em intervalos regulares. Isso garante que a sua pipeline de dados seja executada automaticamente.","title":"4. Agendamento do Workflow"},{"location":"pipeline/#conclusao","text":"Este documento forneceu uma vis\u00e3o geral da cria\u00e7\u00e3o de uma pipeline de dados, incluindo o tratamento das camadas landing, bronze, silver e gold, e a organiza\u00e7\u00e3o do workflow via Databricks. Seguindo estas etapas, voc\u00ea pode criar uma pipeline de dados robusta e eficiente.","title":"Conclus\u00e3o"},{"location":"projeto/","text":"Projeto Final Engenharia de Dados - SATC Tecnologias Utilizadas - Python: Linguagem de programa\u00e7\u00e3o utilizada para desenvolvimento dos scripts. - Azure ADLS2: O Azure Data Lake Storage \u00e9 um conjunto de funcionalidades dedicadas \u00e0 an\u00e1lise de Big Data, Armazenamento de Blobs do Azure. - Databricks: Ferramenta para organiza\u00e7\u00e3o do workflow. - SQL Server: Banco de dados relacional utilizado para armazenamento dos dados. - PySpark: Biblioteca de processamento de dados distribu\u00eddos. Estrutura do Projeto \u251c\u2500 .gitignore \u251c\u2500 LICENSE.md \u251c\u2500 README.md \u251c\u2500 css \u2502 \u2514\u2500 extra.css \u251c\u2500 data \u2502 \u2514\u2500 raw \u2502 \u251c\u2500 dump.sql \u2502 \u251c\u2500 sakila-data.sql \u2502 \u251c\u2500 sakila-schema.sql \u2502 \u2514\u2500 schema.sql \u251c\u2500 desenho_arquitetura.excalidraw \u251c\u2500 desenho_arquitetura.png \u251c\u2500 docs \u2502 \u251c\u2500 docs \u2502 \u2502 \u251c\u2500 index.md \u2502 \u2502 \u251c\u2500 pipeline.md \u2502 \u2502 \u2514\u2500 projeto.md \u2502 \u2514\u2500 mkdocs.yml \u251c\u2500 iac \u2502 \u251c\u2500 .gitignore \u2502 \u251c\u2500 adls \u2502 \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u2502 \u251c\u2500 main.tf \u2502 \u2502 \u251c\u2500 output.tf \u2502 \u2502 \u251c\u2500 provider.tf \u2502 \u2502 \u2514\u2500 variables.tf \u2502 \u251c\u2500 databricks \u2502 \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u2502 \u251c\u2500 main.tf \u2502 \u2502 \u251c\u2500 output.tf \u2502 \u2502 \u251c\u2500 provider.tf \u2502 \u2502 \u2514\u2500 variables.tf \u2502 \u2514\u2500 sqlserver \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u251c\u2500 main.tf \u2502 \u251c\u2500 outputs.tf \u2502 \u251c\u2500 providers.tf \u2502 \u2514\u2500 variables.tf \u251c\u2500 notebooks \u2502 \u251c\u2500 camadas \u2502 \u2502 \u251c\u2500 1-landing.ipynb \u2502 \u2502 \u251c\u2500 2-bronze.ipynb \u2502 \u2502 \u251c\u2500 3-silver.ipynb \u2502 \u2502 \u251c\u2500 4-gold.ipynb \u2502 \u2502 \u251c\u2500 all.py \u2502 \u2502 \u2514\u2500 env.ipynb \u2502 \u2514\u2500 gerador \u2502 \u251c\u2500 .python-version \u2502 \u251c\u2500 gerador.py \u2502 \u2514\u2500 requirements.txt \u2514\u2500 requirements.txt","title":"Projeto"},{"location":"projeto/#projeto-final-engenharia-de-dados-satc","text":"","title":"Projeto Final Engenharia de Dados - SATC"},{"location":"projeto/#tecnologias-utilizadas","text":"- Python: Linguagem de programa\u00e7\u00e3o utilizada para desenvolvimento dos scripts. - Azure ADLS2: O Azure Data Lake Storage \u00e9 um conjunto de funcionalidades dedicadas \u00e0 an\u00e1lise de Big Data, Armazenamento de Blobs do Azure. - Databricks: Ferramenta para organiza\u00e7\u00e3o do workflow. - SQL Server: Banco de dados relacional utilizado para armazenamento dos dados. - PySpark: Biblioteca de processamento de dados distribu\u00eddos.","title":"Tecnologias Utilizadas"},{"location":"projeto/#estrutura-do-projeto","text":"\u251c\u2500 .gitignore \u251c\u2500 LICENSE.md \u251c\u2500 README.md \u251c\u2500 css \u2502 \u2514\u2500 extra.css \u251c\u2500 data \u2502 \u2514\u2500 raw \u2502 \u251c\u2500 dump.sql \u2502 \u251c\u2500 sakila-data.sql \u2502 \u251c\u2500 sakila-schema.sql \u2502 \u2514\u2500 schema.sql \u251c\u2500 desenho_arquitetura.excalidraw \u251c\u2500 desenho_arquitetura.png \u251c\u2500 docs \u2502 \u251c\u2500 docs \u2502 \u2502 \u251c\u2500 index.md \u2502 \u2502 \u251c\u2500 pipeline.md \u2502 \u2502 \u2514\u2500 projeto.md \u2502 \u2514\u2500 mkdocs.yml \u251c\u2500 iac \u2502 \u251c\u2500 .gitignore \u2502 \u251c\u2500 adls \u2502 \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u2502 \u251c\u2500 main.tf \u2502 \u2502 \u251c\u2500 output.tf \u2502 \u2502 \u251c\u2500 provider.tf \u2502 \u2502 \u2514\u2500 variables.tf \u2502 \u251c\u2500 databricks \u2502 \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u2502 \u251c\u2500 main.tf \u2502 \u2502 \u251c\u2500 output.tf \u2502 \u2502 \u251c\u2500 provider.tf \u2502 \u2502 \u2514\u2500 variables.tf \u2502 \u2514\u2500 sqlserver \u2502 \u251c\u2500 .terraform.lock.hcl \u2502 \u251c\u2500 main.tf \u2502 \u251c\u2500 outputs.tf \u2502 \u251c\u2500 providers.tf \u2502 \u2514\u2500 variables.tf \u251c\u2500 notebooks \u2502 \u251c\u2500 camadas \u2502 \u2502 \u251c\u2500 1-landing.ipynb \u2502 \u2502 \u251c\u2500 2-bronze.ipynb \u2502 \u2502 \u251c\u2500 3-silver.ipynb \u2502 \u2502 \u251c\u2500 4-gold.ipynb \u2502 \u2502 \u251c\u2500 all.py \u2502 \u2502 \u2514\u2500 env.ipynb \u2502 \u2514\u2500 gerador \u2502 \u251c\u2500 .python-version \u2502 \u251c\u2500 gerador.py \u2502 \u2514\u2500 requirements.txt \u2514\u2500 requirements.txt","title":"Estrutura do Projeto"}]}